{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiLabelCNN(TL_to_Section).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilverQ/Multi-Label-Text-Classification-master/blob/master/MultiLabelCNN(TL_to_Section).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMKrmyl3NFaV",
        "colab_type": "code",
        "outputId": "afa00a7e-4393-43cc-aecb-04c21c49f5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcgIo3gkNGPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = './gdrive/My Drive/Colab Notebooks/multilabelcnn/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osiv6xF3NHae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_in_path = base_path + 'input_data/'\n",
        "test_data_path = base_path + 'test_data/'\n",
        "data_out_path = base_path + 'result_tl_section/'\n",
        "meta_data_path = base_path + 'meta_data/'\n",
        "vocab_file = meta_data_path + 'vocab.voc'\n",
        "label_file = meta_data_path + 'labels.pickle'\n",
        "freq_file = meta_data_path + 'word_freq.pickle'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr-z3JTpSvmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if not os.path.exists(data_in_path):\n",
        "    os.makedirs(data_in_path)\n",
        "if not os.path.exists(test_data_path):\n",
        "    os.makedirs(test_data_path)\n",
        "if not os.path.exists(data_out_path):\n",
        "    os.makedirs(data_out_path)\n",
        "if not os.path.exists(meta_data_path):\n",
        "    os.makedirs(meta_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noybu73CY98W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import zipfile\n",
        "# train_zip_ref = zipfile.ZipFile(data_in_path + 'input_data.zip', 'r')\n",
        "# train_zip_ref.extractall(data_in_path)\n",
        "# train_zip_ref.close()\n",
        "\n",
        "# test_zip_ref = zipfile.ZipFile(test_data_path + 'test_data.zip', 'r')\n",
        "# test_zip_ref.extractall(test_data_path)\n",
        "# test_zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhMpfUhhNbOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.backend as K\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgZecaXvOVnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiuHfBPlPBKv",
        "colab_type": "code",
        "outputId": "a32e3d3a-8136-4394-ba1c-bff1ae125d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "tr_file_list = os.listdir(data_in_path)\n",
        "tr_file_list = [file for file in tr_file_list if file.endswith(\".txt\")]\n",
        "# tr_file_list = [file for file in tr_file_list if file.startswith(\"cpc\")]\n",
        "\n",
        "test_file_list = os.listdir(test_data_path)\n",
        "test_file_list = [file for file in test_file_list if file.endswith(\".txt\")]\n",
        "print(tr_file_list, '\\n', test_file_list)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cpc_json_0_3docs_01.txt', 'cpc_json_0_3docs_02.txt', 'w_dhhan_1007_00.txt', 'w_dhhan_1007_01.txt', 'w_dhhan_1007_02.txt', 'w_dhhan_1007_03.txt', 'w_dhhan_1007_04.txt', 'w_dhhan_1007_05.txt', 'w_dhhan_1007_06.txt', 'w_dhhan_1007_07.txt', 'w_dhhan_1007_08.txt'] \n",
            " ['w_dhhan_1007_09.txt', 'w_dhhan_1007_10.txt', 'w_dhhan_1007_11.txt', 'w_dhhan_1007_12.txt', 'w_dhhan_1007_13.txt', 'w_dhhan_1007_15.txt', 'w_dhhan_1007_14.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f06KDLC2QBHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "\n",
        "    def __init__(self, train_path, test_path, is_shuffle, train_bs, test_bs, epoch, max_length):\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.is_shuffle = is_shuffle\n",
        "        self.train_bs = train_bs\n",
        "        self.test_bs = test_bs\n",
        "        self.epoch = epoch\n",
        "        self.max_length = max_length\n",
        "        self.special_tokens = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
        "        # self.label_path = label_path\n",
        "        # self.vocab_path = vocab_path\n",
        "\n",
        "        if not os.path.exists(vocab_file):\n",
        "            print('No vocabulary.')\n",
        "            print('Making vocabulary.')\n",
        "            self.build_vocab_by_patent(vocab_file)\n",
        "            print('Complete build vocabulary!')\n",
        "\n",
        "        # print('Loading vocabulary...')\n",
        "        self.idx2word, self.word2idx = pickle.load(open(vocab_file, 'rb'))\n",
        "        print('Successfully load vocabulary!')\n",
        "        self.idx2label, self.label2idx = pickle.load(open(label_file, 'rb'))\n",
        "        print('Successfully load labels')\n",
        "\n",
        "    def build_freq(self, word_list):\n",
        "        word_counts = Counter(word_list)\n",
        "        # print('word_list: ', len(word_list), word_list)\n",
        "        # print('word_counts_1: ', len(word_counts), word_counts)\n",
        "        # print('word_counts_2: ', len(word_counts.most_common()), word_counts.most_common())\n",
        "        freq = Counter()\n",
        "        # freq_file = raw_path + '/word_freq.pickle'\n",
        "        # print(freq_file)\n",
        "        if os.path.exists(freq_file):\n",
        "            with open(freq_file, 'rb') as freq_dist_f:\n",
        "                freq = pickle.load(freq_dist_f)\n",
        "                print('frequency distribution loaded', len(freq))\n",
        "        for word, cnt in word_counts.items():\n",
        "            # print(word, freq[word])\n",
        "            freq[word] += cnt\n",
        "            # print(word, freq[word])\n",
        "        print('freq len: ', len(freq))\n",
        "        with open(freq_file, 'wb') as freq_dist_f:\n",
        "            pickle.dump(freq, freq_dist_f)\n",
        "        return freq\n",
        "\n",
        "    def build_vocab_by_patent(self, vocab_file):\n",
        "        error_cnt = 0\n",
        "        label_list = []\n",
        "        for file in self.train_path:\n",
        "            word_list = []\n",
        "            with open(data_in_path + file, encoding='utf-8') as f:\n",
        "                for line in tqdm(f):\n",
        "                    try:\n",
        "                        # print('line: ', line)\n",
        "                        patent = json.loads(line)\n",
        "                        text = re.sub('[-=.#/?:$}(){,]', ' ', patent['title'] + patent['ab'])\n",
        "                        token = text.split()\n",
        "                        # token = tokenizer(patent['title'])\n",
        "                        # print('token: ', token)\n",
        "                        # doc = en.tokenizer(patent['title']+patent['ab']+patent['cl'])\n",
        "                        labels = patent['cpc'].split('|')\n",
        "                        for tok in token:\n",
        "                            word_list.append(tok.lower())\n",
        "                        labels = [label[0] for label in labels]\n",
        "                        for label in labels:\n",
        "                            if label not in label_list:\n",
        "                                label_list.append(label)\n",
        "                    except:\n",
        "                        error_cnt += 1\n",
        "                        # print('error: ', line)\n",
        "            print('\\nIn \"%s\" word_list: %d, error_cnt: %d\\n' % (file, len(word_list), error_cnt))\n",
        "            idx2word = self.build_freq(word_list)\n",
        "        idx2word = self.special_tokens + [word for word, _ in idx2word.most_common()]\n",
        "        print('idx2word: ', len(idx2word), idx2word[:10])\n",
        "        print('idx2label: ', len(label_list), label_list)\n",
        "        word2idx = {word: idx for idx, word in enumerate(idx2word)}\n",
        "        label2idx = {label: idx for idx, label in enumerate(label_list)}\n",
        "        vocab = (idx2word, word2idx)\n",
        "        label = (label_list, label2idx)\n",
        "        pickle.dump(vocab, open(vocab_file, 'wb'))\n",
        "        pickle.dump(label, open(label_file, 'wb'))\n",
        "\n",
        "    def text_to_sequence(self, text_list):\n",
        "        sequences = []\n",
        "        for text in text_list:\n",
        "            sequences.append([self.word2idx[word] for word in text if word in self.word2idx.keys()])\n",
        "        return sequences\n",
        "\n",
        "    def sequence_to_text(self, sequence):\n",
        "        return [self.idx2word[idx] for idx in sequence if idx != 0]\n",
        "\n",
        "    def read_lines(self, indices, path):\n",
        "        line_count = 0\n",
        "        texts = []\n",
        "        labels = []\n",
        "        # print('indices: ', indices)\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line_count in indices:\n",
        "                    try:\n",
        "                        patent = json.loads(line)\n",
        "                        # text = re.sub('[-=.#/?:$}(){,]', ' ', patent['title'] + patent['ab'])\n",
        "                        text = re.sub('[-=.#/?:$}(){,]', ' ', patent['title'])\n",
        "                        label = patent['cpc'].split('|')\n",
        "                        texts.append(text.lower().split())\n",
        "                        labels.append(list(set([cpc[0] for cpc in label])))\n",
        "                    except:\n",
        "                        print(line)\n",
        "                        print(line_count)\n",
        "                line_count += 1\n",
        "        return texts, labels\n",
        "\n",
        "    def create_multiplehot_labels(self, labels_index):\n",
        "        labels = []\n",
        "        # print(len(label))\n",
        "        for batch in labels_index:\n",
        "            label = [0] * len(self.label2idx)\n",
        "            # print(item)\n",
        "            for cpc in batch:\n",
        "                label[self.label2idx[cpc]] = 1\n",
        "            labels.append(label)\n",
        "        # print('label_repr: ', labels)\n",
        "        return labels\n",
        "\n",
        "    def data_generator(self, is_train):\n",
        "        if is_train:\n",
        "            batch_size = self.train_bs\n",
        "            is_shuffle = self.is_shuffle  # 셔플을 여기서 해줘야해. 밖에서는 느려\n",
        "            file_list = tr_file_list\n",
        "            path = data_in_path\n",
        "        else:\n",
        "            batch_size = self.test_bs\n",
        "            is_shuffle = False\n",
        "            file_list = test_file_list\n",
        "            path = test_data_path\n",
        "        # print(file_list)\n",
        "        for file in tqdm(file_list):\n",
        "            cur_file = path + file\n",
        "            # print(file)\n",
        "            with open(cur_file, encoding='utf-8') as f:  # 일단 읽어서 길이는 알아둔다.\n",
        "                data_length = len(f.readlines())\n",
        "                # print('Num of pat: ', data_length)\n",
        "\n",
        "            indices = list(range(data_length))  # 인덱스를 미리 만들어주는게 제너레이터 사용의 핵심.\n",
        "            if is_shuffle:\n",
        "                shuffle(indices)  # 셔플할꺼라면 이걸... 내장 라이브러리 random에 있는 함수.\n",
        "                # print('suffled indices: ', indices)\n",
        "            current_count = 0\n",
        "            # while True:\n",
        "            #     if current_count >= data_length:\n",
        "            #         return\n",
        "            #     else:\n",
        "            while current_count < data_length:\n",
        "                target_indices = indices[current_count:current_count + batch_size]\n",
        "                texts, labels = self.read_lines(target_indices, cur_file)\n",
        "                tokenized_title = texts\n",
        "                labels = self.create_multiplehot_labels(labels)\n",
        "                indexed_encoder_inputs = self.text_to_sequence(tokenized_title)\n",
        "                padded_encoder_inputs = pad_sequences(indexed_encoder_inputs,\n",
        "                                                      maxlen=self.max_length,\n",
        "                                                      padding='pre')\n",
        "                # print(padded_encoder_inputs, labels)\n",
        "                current_count += batch_size\n",
        "                yield padded_encoder_inputs, labels\n",
        "\n",
        "    def mapping_fn(self, x, y=None):\n",
        "        inputs, label = {'x': x}, y\n",
        "        return inputs, label\n",
        "\n",
        "    def train_input_fn(self):\n",
        "        dataset = tf.data.Dataset.from_generator(generator=lambda: self.data_generator(is_train=True),\n",
        "                                                 output_types=(tf.int64, tf.int64),\n",
        "                                                 output_shapes=(\n",
        "                                                     (None, self.max_length),  # 넣어주면 graph그릴때 잘못 들어온 입력을 잡아줄 수 있다.\n",
        "                                                     (None, None)))  # labels count: unknown\n",
        "        dataset = dataset.map(self.mapping_fn)\n",
        "        dataset = dataset.repeat(count=self.epoch)\n",
        "        return dataset\n",
        "\n",
        "    def test_input_fn(self):\n",
        "        dataset = tf.data.Dataset.from_generator(generator=lambda: self.data_generator(is_train=False),\n",
        "                                                 output_types=(tf.int64, tf.int64),\n",
        "                                                 output_shapes=((None, self.max_length),\n",
        "                                                                (None, None)))\n",
        "        dataset = dataset.map(self.mapping_fn)\n",
        "        return dataset\n",
        "\n",
        "    def eval_input_fn(self):\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            generator=lambda: self.data_generator(is_train=False),\n",
        "            output_types=(tf.int64, tf.int64),\n",
        "            output_shapes=((None, self.max_length), (None, None)))\n",
        "        dataset = dataset.map(self.mapping_fn)\n",
        "        return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJfTZ6TwMc4J",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUUi5HvxRycZ",
        "colab_type": "code",
        "outputId": "5dad77c2-a97a-41c3-b04a-4b1e19420f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset = Dataset(train_path=tr_file_list,\n",
        "                  test_path=test_file_list,\n",
        "                  is_shuffle=True,\n",
        "                  train_bs=500,\n",
        "                  test_bs=100,\n",
        "                  epoch=10,\n",
        "                  max_length=10)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully load vocabulary!\n",
            "Successfully load labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8xTmwnu1Kih",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVjThn5VJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyper_params = {'vocab_size': len(dataset.word2idx),     # or 50,000 or 445,694\n",
        "                'label_size': len(dataset.label2idx),\n",
        "                'embedding_dimension': 256,\n",
        "                'teacher_forcing_rate': 0.5,\n",
        "                'use_attention': True}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z1Ql4rl73kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "    # feature['x'] => (bs, 20)\n",
        "\n",
        "    train_op = features\n",
        "    loss = features\n",
        "    predicted_token = features\n",
        "    embedding_layer = tf.keras.layers.Embedding(params['vocab_size'],\n",
        "                                                params['embedding_dimension'])(features['x'])  # (bs, 20, EMD_SIZE)\n",
        "\n",
        "    dropout_emb = tf.keras.layers.Dropout(rate=0.5)(embedding_layer)  # (bs, 20, EMD_SIZE)\n",
        "\n",
        "    filter_sizes = [3, 4, 5]\n",
        "    pooled_outputs = []\n",
        "    for filter_size in filter_sizes:\n",
        "        conv = tf.keras.layers.Conv1D(\n",
        "            filters=100,\n",
        "            kernel_size=filter_size,\n",
        "            padding='valid',\n",
        "            activation=tf.nn.relu,\n",
        "            kernel_constraint=tf.keras.constraints.max_norm(3.))(dropout_emb)  # (bs, 20, 100)\n",
        "        # 최대 norm 지정, weight clipping이 바로 이 부분\n",
        "\n",
        "        pool = tf.keras.layers.GlobalMaxPool1D()(conv)  # [(bs, 100), (bs, 100), (bs, 100)]\n",
        "        pooled_outputs.append(pool)\n",
        "\n",
        "    h_pool = tf.concat(pooled_outputs, axis=1)  # (bs, 300)\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(units=250, activation=tf.nn.relu,\n",
        "                                   kernel_constraint=tf.keras.constraints.max_norm(3.))(h_pool)  # (bs, 200)\n",
        "    dropout_hidden = tf.keras.layers.Dropout(rate=0.5)(hidden, training=TRAIN)\n",
        "    # logits = tf.keras.layers.Dense(units=1)(dropout_hidden)  # sigmoid를 해주겠다  # (bs, 1)\n",
        "    logits = tf.keras.layers.Dense(units=params['label_size'])(dropout_hidden)  # 이렇게하면 one-hot 필요\n",
        "\n",
        "    if TRAIN:\n",
        "        global_step = tf.train.get_global_step()\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits\n",
        "                                               ,\n",
        "                                               weights=1.0, label_smoothing=0.1\n",
        "                                               )\n",
        "        # loss = tf.losses.softmax_cross_entropy(labels, logits)\n",
        "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
        "        pred = tf.nn.sigmoid(logits)\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        precision = tf.metrics.precision(labels, tf.round(pred))\n",
        "        recall = tf.metrics.recall(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, \n",
        "                                          train_op=train_op,\n",
        "                                          loss=loss,\n",
        "                                          eval_metric_ops={'acc': accuracy, \n",
        "                                                           'prec': precision,\n",
        "                                                           'recall': recall})\n",
        "\n",
        "    elif EVAL:\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "                                              #  ,\n",
        "                                              #  weights=1.0, label_smoothing=0.01)\n",
        "        pred = tf.nn.sigmoid(logits)\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        precision = tf.metrics.precision(labels, tf.round(pred))\n",
        "        recall = tf.metrics.recall(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, \n",
        "                                          loss=loss, \n",
        "                                          eval_metric_ops={'acc': accuracy, \n",
        "                                                           'prec': precision,\n",
        "                                                           'recall': recall})\n",
        "\n",
        "    elif PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\n",
        "                'prob': tf.nn.sigmoid(logits),\n",
        "            }\n",
        "        )\n",
        "    plot_model(model, to_file=data_out_path + 'model.png')\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        loss=loss,\n",
        "        predictions={'prediction': predicted_token})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtsgOYum7-hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeSSMUbjCCPO",
        "colab_type": "code",
        "outputId": "49529428-dee1-4350-fb1d-b68c5c0a5205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "est = tf.estimator.Estimator(model_fn=model_fn,\n",
        "                             params=hyper_params,\n",
        "                             model_dir=data_out_path)\n",
        "# tf.estimator.train_and_evaluate(model_fn, train_spec, eval_spec)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': './gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f943e67edd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN_EWVwtTqg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_spec = tf.estimator.TrainSpec(input_fn=dataset.train_input_fn, max_steps=1000)\n",
        "# eval_spec = tf.estimator.EvalSpec(input_fn=dataset.eval_input_fn, steps=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_vRUSIN8C0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "f58ef068-06d0-447d-ca87-0483d1997b4c"
      },
      "source": [
        "est.train(dataset.train_input_fn)\n",
        "# tf.estimator.train_and_evaluate(model_fn, train_spec, eval_spec)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6917918, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 1/11 [00:00<00:03,  3.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 2/11 [00:00<00:02,  3.74it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.283072\n",
            "INFO:tensorflow:loss = 0.39744103, step = 100 (353.284 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 172 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.278485\n",
            "INFO:tensorflow:loss = 0.35965917, step = 200 (359.072 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 3/11 [11:56<28:38, 214.86s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.269111\n",
            "INFO:tensorflow:loss = 0.36394724, step = 300 (371.593 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 334 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.26683\n",
            "INFO:tensorflow:loss = 0.35004145, step = 400 (374.774 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 36%|███▋      | 4/11 [24:22<43:40, 374.32s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 493 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.257985\n",
            "INFO:tensorflow:loss = 0.34016272, step = 500 (387.619 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.274342\n",
            "INFO:tensorflow:loss = 0.34362793, step = 600 (364.507 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 5/11 [36:54<48:46, 487.67s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BiD9rox8EKf",
        "colab_type": "code",
        "outputId": "3812d0e3-58ec-4c5c-d6b1-6aae711a2178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "valid = est.evaluate(dataset.eval_input_fn, steps=10)\n",
        "# acc = 0.7144097, global_step = 96876, loss = 6.5086164, prec = 0.10945274, recall = 0.12790698\n",
        "\n",
        "label smoothing 0.1 적용 후\n",
        "# acc = 0.7065972, global_step = 121052, loss = 0.7922439, prec = 0.13913043, recall = 0.18604651\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-10-22T16:08:29Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt-121052\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [1/10]\n",
            "INFO:tensorflow:Evaluation [2/10]\n",
            "INFO:tensorflow:Evaluation [3/10]\n",
            "INFO:tensorflow:Evaluation [4/10]\n",
            "INFO:tensorflow:Evaluation [5/10]\n",
            "INFO:tensorflow:Evaluation [6/10]\n",
            "INFO:tensorflow:Evaluation [7/10]\n",
            "INFO:tensorflow:Evaluation [8/10]\n",
            "INFO:tensorflow:Evaluation [9/10]\n",
            "INFO:tensorflow:Evaluation [10/10]\n",
            "INFO:tensorflow:Finished evaluation at 2019-10-22-16:08:57\n",
            "INFO:tensorflow:Saving dict for global step 121052: acc = 0.7065972, global_step = 121052, loss = 0.7922439, prec = 0.13913043, recall = 0.18604651\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 121052: ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt-121052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkQ7d3ejByaa",
        "colab_type": "code",
        "outputId": "3dbf5051-5dc2-472e-d9c3-a7c6ab917df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred_results = est.predict(input_fn=dataset.eval_input_fn)\n",
        "print(pred_results)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Estimator.predict at 0x7fc703909360>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb4HqSwB_4-k",
        "colab_type": "code",
        "outputId": "0cefee25-ff55-4f23-b2a9-1459c1a25a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "# test_output = [pred['prob'] for item in list(pred_results)]\n",
        "test_output = [item for item in list(pred_results)]\n",
        "# test_output = np.array(test_output)\n",
        "print(test_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt-121052\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85FB61COU0aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MDphEkXf5A",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/guide/estimator\n",
        "\n",
        "https://colab.research.google.com/drive/130zRZLtZu8ceWfHmRqQfai09MuAW9fAY#scrollTo=5HeTOvCYbjZb\n",
        "이거 보면서 잘 공부해보쟈~"
      ]
    }
  ]
}