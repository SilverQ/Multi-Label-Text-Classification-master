{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiLabelCNN(ab_to_Subclass)_wo_smooth.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilverQ/Multi-Label-Text-Classification-master/blob/master/MultiLabelCNN(ab_to_Subclass)_wo_smooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMKrmyl3NFaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "a61d2232-efb9-4ff6-8c80-323615df0cc5"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcgIo3gkNGPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = './gdrive/My Drive/Colab Notebooks/multilabelcnn/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osiv6xF3NHae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_in_path = base_path + 'input_data/'\n",
        "test_data_path = base_path + 'test_data/'\n",
        "data_out_path = base_path + 'result_08/'\n",
        "meta_data_path = base_path + 'meta_data/'\n",
        "vocab_file = meta_data_path + 'vocab.voc'\n",
        "label_file = meta_data_path + 'labels_subclass.pickle'\n",
        "freq_file = meta_data_path + 'word_freq.pickle'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr-z3JTpSvmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if not os.path.exists(data_in_path):\n",
        "    os.makedirs(data_in_path)\n",
        "if not os.path.exists(test_data_path):\n",
        "    os.makedirs(test_data_path)\n",
        "if not os.path.exists(data_out_path):\n",
        "    os.makedirs(data_out_path)\n",
        "if not os.path.exists(meta_data_path):\n",
        "    os.makedirs(meta_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noybu73CY98W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import zipfile\n",
        "# train_zip_ref = zipfile.ZipFile(data_in_path + 'input_data.zip', 'r')\n",
        "# train_zip_ref.extractall(data_in_path)\n",
        "# train_zip_ref.close()\n",
        "\n",
        "# test_zip_ref = zipfile.ZipFile(test_data_path + 'test_data.zip', 'r')\n",
        "# test_zip_ref.extractall(test_data_path)\n",
        "# test_zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhMpfUhhNbOV",
        "colab_type": "code",
        "outputId": "eb22533a-281b-49af-89b3-0c5aaee42df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.backend as K\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgZecaXvOVnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiuHfBPlPBKv",
        "colab_type": "code",
        "outputId": "9c510785-e001-454d-c1ac-468f069e2e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tr_file_list = os.listdir(data_in_path)\n",
        "tr_file_list = [file for file in tr_file_list if file.endswith(\".txt\")]\n",
        "# tr_file_list = [file for file in tr_file_list if file.startswith(\"cpc\")]\n",
        "\n",
        "test_file_list = os.listdir(test_data_path)\n",
        "test_file_list = [file for file in test_file_list if file.endswith(\".txt\")]\n",
        "print(tr_file_list, '\\n', test_file_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['w_dhhan_1007_00.txt', 'w_dhhan_1007_01.txt', 'w_dhhan_1007_02.txt', 'w_dhhan_1007_03.txt', 'w_dhhan_1007_04.txt', 'w_dhhan_1007_05.txt', 'w_dhhan_1007_06.txt', 'w_dhhan_1007_07.txt', 'w_dhhan_1007_08.txt'] \n",
            " ['w_dhhan_1007_09.txt', 'w_dhhan_1007_10.txt', 'w_dhhan_1007_11.txt', 'w_dhhan_1007_12.txt', 'w_dhhan_1007_13.txt', 'w_dhhan_1007_15.txt', 'w_dhhan_1007_14.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f06KDLC2QBHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "\n",
        "    def __init__(self, train_path, test_path, is_shuffle, train_bs, test_bs, epoch, max_length):\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.is_shuffle = is_shuffle\n",
        "        self.train_bs = train_bs\n",
        "        self.test_bs = test_bs\n",
        "        self.epoch = epoch\n",
        "        self.max_length = max_length\n",
        "        self.special_tokens = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
        "        # self.label_path = label_path\n",
        "        # self.vocab_path = vocab_path\n",
        "\n",
        "        if not os.path.exists(vocab_file):\n",
        "            print('No vocabulary.')\n",
        "            print('Making vocabulary.')\n",
        "            self.build_vocab_by_patent(vocab_file)\n",
        "            print('Complete build vocabulary!')\n",
        "\n",
        "        if not os.path.exists(label_file):\n",
        "            print('No labels.')\n",
        "            print('Making labels.')\n",
        "            self.build_labels()\n",
        "            print('Complete build labels!')\n",
        "\n",
        "        # print('Loading vocabulary...')\n",
        "        self.idx2word, self.word2idx = pickle.load(open(vocab_file, 'rb'))\n",
        "        print('Successfully load vocabulary!')\n",
        "        self.idx2label, self.label2idx = pickle.load(open(label_file, 'rb'))\n",
        "        print('Successfully load labels')\n",
        "\n",
        "    def build_labels(self):\n",
        "        error_cnt = 0\n",
        "        label_list = []\n",
        "        for file in self.train_path:\n",
        "            with open(data_in_path + file, encoding='utf-8') as f:\n",
        "                for line in tqdm(f):\n",
        "                    try:\n",
        "                        patent = json.loads(line)\n",
        "                        labels = patent['cpc'].split('|')\n",
        "                        labels = [label[:4] for label in labels]\n",
        "                        for label in labels:\n",
        "                            if label not in label_list:\n",
        "                                label_list.append(label)\n",
        "                    except:\n",
        "                        error_cnt += 1\n",
        "                        # print('error: ', line)\n",
        "            # print('\\nIn \"%s\" word_list: %d, error_cnt: %d\\n' % (file, len(word_list), error_cnt))\n",
        "        print('idx2label: ', len(label_list), label_list)\n",
        "        label2idx = {label: idx for idx, label in enumerate(label_list)}\n",
        "        label = (label_list, label2idx)\n",
        "        pickle.dump(label, open(label_file, 'wb'))\n",
        "\n",
        "    def build_freq(self, word_list):\n",
        "        word_counts = Counter(word_list)\n",
        "        # print('word_list: ', len(word_list), word_list)\n",
        "        # print('word_counts_1: ', len(word_counts), word_counts)\n",
        "        # print('word_counts_2: ', len(word_counts.most_common()), word_counts.most_common())\n",
        "        freq = Counter()\n",
        "        # freq_file = raw_path + '/word_freq.pickle'\n",
        "        # print(freq_file)\n",
        "        if os.path.exists(freq_file):\n",
        "            with open(freq_file, 'rb') as freq_dist_f:\n",
        "                freq = pickle.load(freq_dist_f)\n",
        "                print('frequency distribution loaded', len(freq))\n",
        "        for word, cnt in word_counts.items():\n",
        "            # print(word, freq[word])\n",
        "            freq[word] += cnt\n",
        "            # print(word, freq[word])\n",
        "        print('freq len: ', len(freq))\n",
        "        with open(freq_file, 'wb') as freq_dist_f:\n",
        "            pickle.dump(freq, freq_dist_f)\n",
        "        return freq\n",
        "\n",
        "    def build_vocab_by_patent(self, vocab_file):\n",
        "        error_cnt = 0\n",
        "        label_list = []\n",
        "        for file in self.train_path:\n",
        "            word_list = []\n",
        "            with open(data_in_path + file, encoding='utf-8') as f:\n",
        "                for line in tqdm(f):\n",
        "                    try:\n",
        "                        # print('line: ', line)\n",
        "                        patent = json.loads(line)\n",
        "                        text = re.sub('[-=.#/?:$}(){,]', ' ', patent['title'] + patent['ab'] + patent['cl'])\n",
        "                        token = text.split()\n",
        "                        # token = tokenizer(patent['title'])\n",
        "                        # print('token: ', token)\n",
        "                        # doc = en.tokenizer(patent['title']+patent['ab']+patent['cl'])\n",
        "                        labels = patent['cpc'].split('|')\n",
        "                        for tok in token:\n",
        "                            word_list.append(tok.lower())\n",
        "                        labels = [label[0] for label in labels]\n",
        "                        for label in labels:\n",
        "                            if label not in label_list:\n",
        "                                label_list.append(label)\n",
        "                    except:\n",
        "                        error_cnt += 1\n",
        "                        # print('error: ', line)\n",
        "            print('\\nIn \"%s\" word_list: %d, error_cnt: %d\\n' % (file, len(word_list), error_cnt))\n",
        "            idx2word = self.build_freq(word_list)\n",
        "        idx2word = self.special_tokens + [word for word, _ in idx2word.most_common(99996)]\n",
        "        print('idx2word: ', len(idx2word), idx2word[:10])\n",
        "        print('idx2label: ', len(label_list), label_list)\n",
        "        word2idx = {word: idx for idx, word in enumerate(idx2word)}\n",
        "        label2idx = {label: idx for idx, label in enumerate(label_list)}\n",
        "        vocab = (idx2word, word2idx)\n",
        "        label = (label_list, label2idx)\n",
        "        pickle.dump(vocab, open(vocab_file, 'wb'))\n",
        "        pickle.dump(label, open(label_file, 'wb'))\n",
        "\n",
        "    def text_to_sequence(self, text_list):\n",
        "        sequences = []\n",
        "        for text in text_list:\n",
        "            sequences.append([self.word2idx[word] for word in text if word in self.word2idx.keys()])\n",
        "        return sequences\n",
        "\n",
        "    def sequence_to_text(self, sequence):\n",
        "        return [self.idx2word[idx] for idx in sequence if idx != 0]\n",
        "\n",
        "    def read_lines(self, indices, path):\n",
        "        line_count = 0\n",
        "        texts = []\n",
        "        labels = []\n",
        "        # print('indices: ', indices)\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line_count in indices:\n",
        "                    try:\n",
        "                        patent = json.loads(line)\n",
        "                        # text = re.sub('[-=.#/?:$}(){,]', ' ', patent['title'] + patent['ab'])\n",
        "                        text = re.sub('[-=.#/?:$}(){,]', ' ', patent['ab'])\n",
        "                        label = patent['cpc'].split('|')\n",
        "                        texts.append(text.lower().split())\n",
        "                        labels.append(list(set([cpc[:4] for cpc in label])))\n",
        "                    except:\n",
        "                      pass\n",
        "                        # print(line)\n",
        "                        # print(line_count)\n",
        "                line_count += 1\n",
        "        return texts, labels\n",
        "\n",
        "    def create_multiplehot_labels(self, labels_index):\n",
        "        labels = []\n",
        "        # print(len(label))\n",
        "        for batch in labels_index:\n",
        "            label = [0] * len(self.label2idx)\n",
        "            # print(item)\n",
        "            for cpc in batch:\n",
        "                label[self.label2idx[cpc]] = 1\n",
        "            labels.append(label)\n",
        "        # print('label_repr: ', labels)\n",
        "        return labels\n",
        "\n",
        "    def data_generator(self, is_train):\n",
        "        if is_train:\n",
        "            batch_size = self.train_bs\n",
        "            is_shuffle = self.is_shuffle  # 셔플을 여기서 해줘야해. 밖에서는 느려\n",
        "            file_list = tr_file_list\n",
        "            path = data_in_path\n",
        "        else:\n",
        "            batch_size = self.test_bs\n",
        "            is_shuffle = False\n",
        "            file_list = test_file_list\n",
        "            path = test_data_path\n",
        "        # print(file_list)\n",
        "        for file in tqdm(file_list):\n",
        "            cur_file = path + file\n",
        "            # print(file)\n",
        "            with open(cur_file, encoding='utf-8') as f:  # 일단 읽어서 길이는 알아둔다.\n",
        "                data_length = len(f.readlines())\n",
        "                # print('Num of pat: ', data_length)\n",
        "\n",
        "            indices = list(range(data_length))  # 인덱스를 미리 만들어주는게 제너레이터 사용의 핵심.\n",
        "            if is_shuffle:\n",
        "                shuffle(indices)  # 셔플할꺼라면 이걸... 내장 라이브러리 random에 있는 함수.\n",
        "                # print('suffled indices: ', indices)\n",
        "            current_count = 0\n",
        "            # while True:\n",
        "            #     if current_count >= data_length:\n",
        "            #         return\n",
        "            #     else:\n",
        "            while current_count < data_length:\n",
        "                target_indices = indices[current_count:current_count + batch_size]\n",
        "                texts, labels = self.read_lines(target_indices, cur_file)\n",
        "                tokenized_title = texts\n",
        "                labels = self.create_multiplehot_labels(labels)\n",
        "                indexed_encoder_inputs = self.text_to_sequence(tokenized_title)\n",
        "                padded_encoder_inputs = pad_sequences(indexed_encoder_inputs,\n",
        "                                                      maxlen=self.max_length,\n",
        "                                                      padding='pre')\n",
        "                # print(padded_encoder_inputs, labels)\n",
        "                current_count += batch_size\n",
        "                yield padded_encoder_inputs, labels\n",
        "\n",
        "    def mapping_fn(self, x, y=None):\n",
        "        inputs, label = {'x': x}, y\n",
        "        return inputs, label\n",
        "\n",
        "    def train_input_fn(self):\n",
        "        dataset = tf.data.Dataset.from_generator(generator=lambda: self.data_generator(is_train=True),\n",
        "                                                 output_types=(tf.int64, tf.int64),\n",
        "                                                 output_shapes=(\n",
        "                                                     (None, self.max_length),  # 넣어주면 graph그릴때 잘못 들어온 입력을 잡아줄 수 있다.\n",
        "                                                     (None, None)))  # labels count: unknown\n",
        "        dataset = dataset.map(self.mapping_fn)\n",
        "        dataset = dataset.repeat(count=self.epoch)\n",
        "        return dataset\n",
        "\n",
        "    def test_input_fn(self):\n",
        "        dataset = tf.data.Dataset.from_generator(generator=lambda: self.data_generator(is_train=False),\n",
        "                                                 output_types=(tf.int64, tf.int64),\n",
        "                                                 output_shapes=((None, self.max_length),\n",
        "                                                                (None, None)))\n",
        "        dataset = dataset.map(self.mapping_fn)\n",
        "        return dataset\n",
        "\n",
        "    def eval_input_fn(self):\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            generator=lambda: self.data_generator(is_train=False),\n",
        "            output_types=(tf.int64, tf.int64),\n",
        "            output_shapes=((None, self.max_length), (None, None)))\n",
        "        dataset = dataset.map(self.mapping_fn)\n",
        "        return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJfTZ6TwMc4J",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUUi5HvxRycZ",
        "colab_type": "code",
        "outputId": "ff3da52a-7bf1-41fd-fbff-2c96dfc1bd32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset = Dataset(train_path=tr_file_list,\n",
        "                  test_path=test_file_list,\n",
        "                  is_shuffle=True,\n",
        "                  train_bs=100,\n",
        "                  test_bs=100,\n",
        "                  epoch=10,\n",
        "                  max_length=150)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully load vocabulary!\n",
            "Successfully load labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8xTmwnu1Kih",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVjThn5VJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyper_params = {'vocab_size': len(dataset.word2idx),     # or 50,000 or 445,694\n",
        "                'label_size': len(dataset.label2idx),\n",
        "                'embedding_dimension': 256,\n",
        "                'teacher_forcing_rate': 0.5,\n",
        "                'use_attention': True}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z1Ql4rl73kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "    # feature['x'] => (bs, 20)\n",
        "\n",
        "    train_op = features\n",
        "    loss = features\n",
        "    predicted_token = features\n",
        "    embedding_layer = tf.keras.layers.Embedding(params['vocab_size'],\n",
        "                                                params['embedding_dimension'])(features['x'])  # (bs, 20, EMD_SIZE)\n",
        "\n",
        "    dropout_emb = tf.keras.layers.Dropout(rate=0.5)(embedding_layer)  # (bs, 20, EMD_SIZE)\n",
        "\n",
        "    filter_sizes = [3, 4, 5]\n",
        "    pooled_outputs = []\n",
        "    for filter_size in filter_sizes:\n",
        "        conv = tf.keras.layers.Conv1D(\n",
        "            filters=100,\n",
        "            kernel_size=filter_size,\n",
        "            padding='valid',\n",
        "            activation=tf.nn.relu,\n",
        "            kernel_constraint=tf.keras.constraints.max_norm(3.))(dropout_emb)  # (bs, 20, 100)\n",
        "        # 최대 norm 지정, weight clipping이 바로 이 부분\n",
        "\n",
        "        pool = tf.keras.layers.GlobalMaxPool1D()(conv)  # [(bs, 100), (bs, 100), (bs, 100)]\n",
        "        pooled_outputs.append(pool)\n",
        "\n",
        "    h_pool = tf.concat(pooled_outputs, axis=1)  # (bs, 300)\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(units=250, activation=tf.nn.relu,\n",
        "                                   kernel_constraint=tf.keras.constraints.max_norm(3.))(h_pool)  # (bs, 200)\n",
        "    dropout_hidden = tf.keras.layers.Dropout(rate=0.5)(hidden, training=TRAIN)\n",
        "    # logits = tf.keras.layers.Dense(units=1)(dropout_hidden)  # sigmoid를 해주겠다  # (bs, 1)\n",
        "    logits = tf.keras.layers.Dense(units=params['label_size'])(dropout_hidden)  # 이렇게하면 one-hot 필요\n",
        "\n",
        "    if TRAIN:\n",
        "        global_step = tf.train.get_global_step()\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits\n",
        "                                               ,\n",
        "                                               weights=1.0, label_smoothing=0.1\n",
        "                                               )\n",
        "        # loss = tf.losses.softmax_cross_entropy(labels, logits)\n",
        "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
        "        pred = tf.nn.sigmoid(logits)\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        precision = tf.metrics.precision(labels, tf.round(pred))\n",
        "        recall = tf.metrics.recall(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, \n",
        "                                          train_op=train_op,\n",
        "                                          loss=loss,\n",
        "                                          eval_metric_ops={'acc': accuracy, \n",
        "                                                           'prec': precision,\n",
        "                                                           'recall': recall})\n",
        "\n",
        "    elif EVAL:\n",
        "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
        "                                              #  ,\n",
        "                                              #  weights=1.0, label_smoothing=0.01)\n",
        "        pred = tf.nn.sigmoid(logits)\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        precision = tf.metrics.precision(labels, tf.round(pred))\n",
        "        recall = tf.metrics.recall(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, \n",
        "                                          loss=loss, \n",
        "                                          eval_metric_ops={'acc': accuracy, \n",
        "                                                           'prec': precision,\n",
        "                                                           'recall': recall})\n",
        "\n",
        "    elif PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\n",
        "                'prob': tf.nn.sigmoid(logits),\n",
        "            }\n",
        "        )\n",
        "    plot_model(model, to_file=data_out_path + 'model.png')\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        loss=loss,\n",
        "        predictions={'prediction': predicted_token})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtsgOYum7-hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeSSMUbjCCPO",
        "colab_type": "code",
        "outputId": "b7670335-ba7e-456b-c2ff-b177c7bcf562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "est = tf.estimator.Estimator(model_fn=model_fn,\n",
        "                             params=hyper_params,\n",
        "                             model_dir=data_out_path)\n",
        "# tf.estimator.train_and_evaluate(model_fn, train_spec, eval_spec)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': './gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fee29ed9a58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN_EWVwtTqg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_spec = tf.estimator.TrainSpec(input_fn=dataset.train_input_fn, max_steps=1000)\n",
        "# eval_spec = tf.estimator.EvalSpec(input_fn=dataset.eval_input_fn, steps=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce93USf01sa9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_vRUSIN8C0B",
        "colab_type": "code",
        "outputId": "5507cf63-f222-4eb6-a794-155e365e0be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "est.train(dataset.train_input_fn)\n",
        "# tf.estimator.train_and_evaluate(model_fn, train_spec, eval_spec)\n",
        "# 20191023에 1epoch만 돌리고 끝냈음."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt-0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6938273, step = 0\n",
            "INFO:tensorflow:global_step/sec: 0.428686\n",
            "INFO:tensorflow:loss = 0.21188895, step = 100 (233.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.426618\n",
            "INFO:tensorflow:loss = 0.21093006, step = 200 (234.401 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 255 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.424904\n",
            "INFO:tensorflow:loss = 0.20941779, step = 300 (235.346 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.433631\n",
            "INFO:tensorflow:loss = 0.20853595, step = 400 (230.611 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.429417\n",
            "INFO:tensorflow:loss = 0.20925444, step = 500 (232.874 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 513 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.421251\n",
            "INFO:tensorflow:loss = 0.20805827, step = 600 (237.388 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.426051\n",
            "INFO:tensorflow:loss = 0.20813014, step = 700 (234.714 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 768 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.42172\n",
            "INFO:tensorflow:loss = 0.20835415, step = 800 (237.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.426228\n",
            "INFO:tensorflow:loss = 0.20742297, step = 900 (234.613 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 1/9 [39:09<5:13:18, 2349.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.417769\n",
            "INFO:tensorflow:loss = 0.20808461, step = 1000 (239.368 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1021 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.419667\n",
            "INFO:tensorflow:loss = 0.20817152, step = 1100 (238.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.428607\n",
            "INFO:tensorflow:loss = 0.20814136, step = 1200 (233.314 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1278 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:global_step/sec: 0.430867\n",
            "INFO:tensorflow:loss = 0.20790572, step = 1300 (232.088 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.427198\n",
            "INFO:tensorflow:loss = 0.2077072, step = 1400 (234.086 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.434233\n",
            "INFO:tensorflow:loss = 0.20760316, step = 1500 (230.290 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1536 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.429129\n",
            "INFO:tensorflow:loss = 0.20790194, step = 1600 (233.031 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.4275\n",
            "INFO:tensorflow:loss = 0.20749888, step = 1700 (233.915 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1792 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.422436\n",
            "INFO:tensorflow:loss = 0.20688674, step = 1800 (236.726 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.421245\n",
            "INFO:tensorflow:loss = 0.20767011, step = 1900 (237.392 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 2/9 [1:18:12<4:33:54, 2347.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.427647\n",
            "INFO:tensorflow:loss = 0.2078384, step = 2000 (233.837 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2047 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.423292\n",
            "INFO:tensorflow:loss = 0.20693012, step = 2100 (236.240 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.425988\n",
            "INFO:tensorflow:loss = 0.20753291, step = 2200 (234.752 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.429691\n",
            "INFO:tensorflow:loss = 0.20674856, step = 2300 (232.725 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2303 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.419532\n",
            "INFO:tensorflow:loss = 0.20669562, step = 2400 (238.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.423151\n",
            "INFO:tensorflow:loss = 0.20680265, step = 2500 (236.324 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2556 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.418214\n",
            "INFO:tensorflow:loss = 0.20613718, step = 2600 (239.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.423984\n",
            "INFO:tensorflow:loss = 0.20781052, step = 2700 (235.856 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.429827\n",
            "INFO:tensorflow:loss = 0.20660423, step = 2800 (232.650 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2812 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.422752\n",
            "INFO:tensorflow:loss = 0.20767687, step = 2900 (236.548 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 3/9 [1:57:36<3:55:14, 2352.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.414882\n",
            "INFO:tensorflow:loss = 0.2075986, step = 3000 (241.032 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3065 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.424547\n",
            "INFO:tensorflow:loss = 0.20736195, step = 3100 (235.543 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.429974\n",
            "INFO:tensorflow:loss = 0.20683685, step = 3200 (232.575 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.428296\n",
            "INFO:tensorflow:loss = 0.20610921, step = 3300 (233.480 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3322 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.427093\n",
            "INFO:tensorflow:loss = 0.20757973, step = 3400 (234.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.433275\n",
            "INFO:tensorflow:loss = 0.2077446, step = 3500 (230.803 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3582 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.432623\n",
            "INFO:tensorflow:loss = 0.20595747, step = 3600 (231.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.442205\n",
            "INFO:tensorflow:loss = 0.20690684, step = 3700 (226.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.43139\n",
            "INFO:tensorflow:loss = 0.20719475, step = 3800 (231.808 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3843 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.42402\n",
            "INFO:tensorflow:loss = 0.20662555, step = 3900 (235.839 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 4/9 [2:36:24<3:15:25, 2345.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.423446\n",
            "INFO:tensorflow:loss = 0.20682724, step = 4000 (236.154 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4100 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.433443\n",
            "INFO:tensorflow:loss = 0.20690595, step = 4100 (230.709 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.437854\n",
            "INFO:tensorflow:loss = 0.20646821, step = 4200 (228.395 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.442879\n",
            "INFO:tensorflow:loss = 0.20794249, step = 4300 (225.791 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4365 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.440073\n",
            "INFO:tensorflow:loss = 0.20617776, step = 4400 (227.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.443251\n",
            "INFO:tensorflow:loss = 0.2069314, step = 4500 (225.604 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.442981\n",
            "INFO:tensorflow:loss = 0.20640841, step = 4600 (225.745 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4631 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.440174\n",
            "INFO:tensorflow:loss = 0.20665853, step = 4700 (227.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.441747\n",
            "INFO:tensorflow:loss = 0.20560798, step = 4800 (226.378 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4896 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.441757\n",
            "INFO:tensorflow:loss = 0.20637223, step = 4900 (226.366 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 5/9 [3:14:19<2:34:56, 2324.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.430597\n",
            "INFO:tensorflow:loss = 0.20766686, step = 5000 (232.237 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.445738\n",
            "INFO:tensorflow:loss = 0.20676674, step = 5100 (224.349 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5159 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.437213\n",
            "INFO:tensorflow:loss = 0.2072473, step = 5200 (228.717 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.445416\n",
            "INFO:tensorflow:loss = 0.20676683, step = 5300 (224.511 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.445232\n",
            "INFO:tensorflow:loss = 0.20729569, step = 5400 (224.602 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5425 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.441042\n",
            "INFO:tensorflow:loss = 0.20670652, step = 5500 (226.737 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.445035\n",
            "INFO:tensorflow:loss = 0.2062394, step = 5600 (224.700 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5691 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.437963\n",
            "INFO:tensorflow:loss = 0.20643938, step = 5700 (228.328 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.444153\n",
            "INFO:tensorflow:loss = 0.20682368, step = 5800 (225.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.438214\n",
            "INFO:tensorflow:loss = 0.20650089, step = 5900 (228.201 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5955 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 6/9 [3:52:08<1:55:23, 2307.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.427883\n",
            "INFO:tensorflow:loss = 0.20621088, step = 6000 (233.707 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.444184\n",
            "INFO:tensorflow:loss = 0.20609604, step = 6100 (225.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.443195\n",
            "INFO:tensorflow:loss = 0.20609786, step = 6200 (225.635 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6218 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.439185\n",
            "INFO:tensorflow:loss = 0.2069326, step = 6300 (227.691 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.446429\n",
            "INFO:tensorflow:loss = 0.20687589, step = 6400 (224.002 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6484 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.438946\n",
            "INFO:tensorflow:loss = 0.20627157, step = 6500 (227.819 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.444411\n",
            "INFO:tensorflow:loss = 0.20590363, step = 6600 (225.015 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.444477\n",
            "INFO:tensorflow:loss = 0.20633052, step = 6700 (224.986 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6750 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.443206\n",
            "INFO:tensorflow:loss = 0.20651063, step = 6800 (225.630 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.446222\n",
            "INFO:tensorflow:loss = 0.20585294, step = 6900 (224.105 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 7/9 [4:29:49<1:16:27, 2293.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.432832\n",
            "INFO:tensorflow:loss = 0.2058479, step = 7000 (231.033 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7014 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.431275\n",
            "INFO:tensorflow:loss = 0.20647302, step = 7100 (231.869 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.433603\n",
            "INFO:tensorflow:loss = 0.20644572, step = 7200 (230.626 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7274 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.426121\n",
            "INFO:tensorflow:loss = 0.20590173, step = 7300 (234.678 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.432935\n",
            "INFO:tensorflow:loss = 0.20583199, step = 7400 (230.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.429743\n",
            "INFO:tensorflow:loss = 0.20570138, step = 7500 (232.700 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7532 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.425383\n",
            "INFO:tensorflow:loss = 0.2053157, step = 7600 (235.083 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.432312\n",
            "INFO:tensorflow:loss = 0.20687738, step = 7700 (231.313 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7790 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.426275\n",
            "INFO:tensorflow:loss = 0.20561163, step = 7800 (234.591 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.430324\n",
            "INFO:tensorflow:loss = 0.20584336, step = 7900 (232.381 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 8/9 [5:08:43<38:25, 2305.58s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.417916\n",
            "INFO:tensorflow:loss = 0.20654717, step = 8000 (239.284 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8045 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.425975\n",
            "INFO:tensorflow:loss = 0.20606473, step = 8100 (234.757 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.427675\n",
            "INFO:tensorflow:loss = 0.20623761, step = 8200 (233.821 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8301 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.426195\n",
            "INFO:tensorflow:loss = 0.20600611, step = 8300 (234.634 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.424582\n",
            "INFO:tensorflow:loss = 0.20556374, step = 8400 (235.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.427199\n",
            "INFO:tensorflow:loss = 0.20576163, step = 8500 (234.084 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8557 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.424079\n",
            "INFO:tensorflow:loss = 0.20549867, step = 8600 (235.806 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.427018\n",
            "INFO:tensorflow:loss = 0.20515403, step = 8700 (234.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.42915\n",
            "INFO:tensorflow:loss = 0.20540442, step = 8800 (233.021 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8813 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.422919\n",
            "INFO:tensorflow:loss = 0.20563653, step = 8900 (236.455 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [5:47:55<00:00, 2319.64s/it]\n",
            "  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.419476\n",
            "INFO:tensorflow:loss = 0.20459607, step = 9000 (238.391 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9067 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.42477\n",
            "INFO:tensorflow:loss = 0.2049332, step = 9100 (235.422 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.424675\n",
            "INFO:tensorflow:loss = 0.20501785, step = 9200 (235.475 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.426609\n",
            "INFO:tensorflow:loss = 0.20396136, step = 9300 (234.408 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9322 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.421014\n",
            "INFO:tensorflow:loss = 0.20420155, step = 9400 (237.519 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.425201\n",
            "INFO:tensorflow:loss = 0.2044816, step = 9500 (235.181 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9577 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.424503\n",
            "INFO:tensorflow:loss = 0.20468141, step = 9600 (235.570 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.426524\n",
            "INFO:tensorflow:loss = 0.20385283, step = 9700 (234.454 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.427725\n",
            "INFO:tensorflow:loss = 0.20436119, step = 9800 (233.798 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9833 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.420627\n",
            "INFO:tensorflow:loss = 0.2040784, step = 9900 (237.740 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 1/9 [39:17<5:14:16, 2357.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.42104\n",
            "INFO:tensorflow:loss = 0.20420521, step = 10000 (237.509 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10087 into ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_08/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.42254\n",
            "INFO:tensorflow:loss = 0.20450763, step = 10100 (236.664 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2OOuJP28esu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BiD9rox8EKf",
        "colab_type": "code",
        "outputId": "9d40d20a-d5d3-4805-cec1-8a8294e49ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "valid = est.evaluate(dataset.eval_input_fn, steps=10)\n",
        "# acc = 0.7144097, global_step = 96876, loss = 6.5086164, prec = 0.10945274, recall = 0.12790698\n",
        "\n",
        "# label smoothing 0.1 적용 후\n",
        "# acc = 0.7065972, global_step = 121052, loss = 0.7922439, prec = 0.13913043, recall = 0.18604651\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-10-25T19:10:55Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_05/model.ckpt-18603\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [1/10]\n",
            "INFO:tensorflow:Evaluation [2/10]\n",
            "INFO:tensorflow:Evaluation [3/10]\n",
            "INFO:tensorflow:Evaluation [4/10]\n",
            "INFO:tensorflow:Evaluation [5/10]\n",
            "INFO:tensorflow:Evaluation [6/10]\n",
            "INFO:tensorflow:Evaluation [7/10]\n",
            "INFO:tensorflow:Evaluation [8/10]\n",
            "INFO:tensorflow:Evaluation [9/10]\n",
            "INFO:tensorflow:Evaluation [10/10]\n",
            "INFO:tensorflow:Finished evaluation at 2019-10-25-19:12:01\n",
            "INFO:tensorflow:Saving dict for global step 18603: acc = 0.91944444, global_step = 18603, loss = 0.22814302, prec = 0.8592189, recall = 0.62401056\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 18603: ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_05/model.ckpt-18603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkQ7d3ejByaa",
        "colab_type": "code",
        "outputId": "3dbf5051-5dc2-472e-d9c3-a7c6ab917df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred_results = est.predict(input_fn=dataset.eval_input_fn)\n",
        "print(pred_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Estimator.predict at 0x7fc703909360>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb4HqSwB_4-k",
        "colab_type": "code",
        "outputId": "0cefee25-ff55-4f23-b2a9-1459c1a25a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "# test_output = [pred['prob'] for item in list(pred_results)]\n",
        "test_output = [item for item in list(pred_results)]\n",
        "# test_output = np.array(test_output)\n",
        "print(test_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Colab Notebooks/multilabelcnn/result_tl_section/model.ckpt-121052\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85FB61COU0aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MDphEkXf5A",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/guide/estimator\n",
        "\n",
        "https://colab.research.google.com/drive/130zRZLtZu8ceWfHmRqQfai09MuAW9fAY#scrollTo=5HeTOvCYbjZb\n",
        "이거 보면서 잘 공부해보쟈~"
      ]
    }
  ]
}